{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math \n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAST_TIME_RANGE = 24  # use 24 past timeframe\n",
    "FUTURE_TIME_RANGE = 6  # predict 6 timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"wikipedia.csv\", index_col=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>0.617284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>0.041002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>-0.581760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>-1.025069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>-1.291514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        views\n",
       "timestamp                    \n",
       "2016-01-01 00:00:00  0.617284\n",
       "2016-01-01 01:00:00  0.041002\n",
       "2016-01-01 02:00:00 -0.581760\n",
       "2016-01-01 03:00:00 -1.025069\n",
       "2016-01-01 04:00:00 -1.291514"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T-0</th>\n",
       "      <th>T-1</th>\n",
       "      <th>T-2</th>\n",
       "      <th>T-3</th>\n",
       "      <th>T-4</th>\n",
       "      <th>T-5</th>\n",
       "      <th>T-6</th>\n",
       "      <th>T-7</th>\n",
       "      <th>T-8</th>\n",
       "      <th>T-9</th>\n",
       "      <th>...</th>\n",
       "      <th>T-20</th>\n",
       "      <th>T-21</th>\n",
       "      <th>T-22</th>\n",
       "      <th>T-23</th>\n",
       "      <th>T+1</th>\n",
       "      <th>T+2</th>\n",
       "      <th>T+3</th>\n",
       "      <th>T+4</th>\n",
       "      <th>T+5</th>\n",
       "      <th>T+6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 23:00:00</th>\n",
       "      <td>0.898701</td>\n",
       "      <td>2.292393</td>\n",
       "      <td>3.798601</td>\n",
       "      <td>3.302904</td>\n",
       "      <td>1.769037</td>\n",
       "      <td>2.001836</td>\n",
       "      <td>1.445126</td>\n",
       "      <td>1.372633</td>\n",
       "      <td>1.309160</td>\n",
       "      <td>1.565804</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.025069</td>\n",
       "      <td>-0.581760</td>\n",
       "      <td>0.041002</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.113880</td>\n",
       "      <td>-0.547334</td>\n",
       "      <td>-0.905009</td>\n",
       "      <td>-1.132992</td>\n",
       "      <td>-1.258555</td>\n",
       "      <td>-1.311624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 00:00:00</th>\n",
       "      <td>0.113880</td>\n",
       "      <td>0.898701</td>\n",
       "      <td>2.292393</td>\n",
       "      <td>3.798601</td>\n",
       "      <td>3.302904</td>\n",
       "      <td>1.769037</td>\n",
       "      <td>2.001836</td>\n",
       "      <td>1.445126</td>\n",
       "      <td>1.372633</td>\n",
       "      <td>1.309160</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.291514</td>\n",
       "      <td>-1.025069</td>\n",
       "      <td>-0.581760</td>\n",
       "      <td>0.041002</td>\n",
       "      <td>-0.547334</td>\n",
       "      <td>-0.905009</td>\n",
       "      <td>-1.132992</td>\n",
       "      <td>-1.258555</td>\n",
       "      <td>-1.311624</td>\n",
       "      <td>-1.219801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 01:00:00</th>\n",
       "      <td>-0.547334</td>\n",
       "      <td>0.113880</td>\n",
       "      <td>0.898701</td>\n",
       "      <td>2.292393</td>\n",
       "      <td>3.798601</td>\n",
       "      <td>3.302904</td>\n",
       "      <td>1.769037</td>\n",
       "      <td>2.001836</td>\n",
       "      <td>1.445126</td>\n",
       "      <td>1.372633</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.395559</td>\n",
       "      <td>-1.291514</td>\n",
       "      <td>-1.025069</td>\n",
       "      <td>-0.581760</td>\n",
       "      <td>-0.905009</td>\n",
       "      <td>-1.132992</td>\n",
       "      <td>-1.258555</td>\n",
       "      <td>-1.311624</td>\n",
       "      <td>-1.219801</td>\n",
       "      <td>-0.935167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 02:00:00</th>\n",
       "      <td>-0.905009</td>\n",
       "      <td>-0.547334</td>\n",
       "      <td>0.113880</td>\n",
       "      <td>0.898701</td>\n",
       "      <td>2.292393</td>\n",
       "      <td>3.798601</td>\n",
       "      <td>3.302904</td>\n",
       "      <td>1.769037</td>\n",
       "      <td>2.001836</td>\n",
       "      <td>1.445126</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.380255</td>\n",
       "      <td>-1.395559</td>\n",
       "      <td>-1.291514</td>\n",
       "      <td>-1.025069</td>\n",
       "      <td>-1.132992</td>\n",
       "      <td>-1.258555</td>\n",
       "      <td>-1.311624</td>\n",
       "      <td>-1.219801</td>\n",
       "      <td>-0.935167</td>\n",
       "      <td>-0.475324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 03:00:00</th>\n",
       "      <td>-1.132992</td>\n",
       "      <td>-0.905009</td>\n",
       "      <td>-0.547334</td>\n",
       "      <td>0.113880</td>\n",
       "      <td>0.898701</td>\n",
       "      <td>2.292393</td>\n",
       "      <td>3.798601</td>\n",
       "      <td>3.302904</td>\n",
       "      <td>1.769037</td>\n",
       "      <td>2.001836</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.233514</td>\n",
       "      <td>-1.380255</td>\n",
       "      <td>-1.395559</td>\n",
       "      <td>-1.291514</td>\n",
       "      <td>-1.258555</td>\n",
       "      <td>-1.311624</td>\n",
       "      <td>-1.219801</td>\n",
       "      <td>-0.935167</td>\n",
       "      <td>-0.475324</td>\n",
       "      <td>-0.013263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          T-0       T-1       T-2       T-3       T-4  \\\n",
       "timestamp                                                               \n",
       "2016-01-01 23:00:00  0.898701  2.292393  3.798601  3.302904  1.769037   \n",
       "2016-01-02 00:00:00  0.113880  0.898701  2.292393  3.798601  3.302904   \n",
       "2016-01-02 01:00:00 -0.547334  0.113880  0.898701  2.292393  3.798601   \n",
       "2016-01-02 02:00:00 -0.905009 -0.547334  0.113880  0.898701  2.292393   \n",
       "2016-01-02 03:00:00 -1.132992 -0.905009 -0.547334  0.113880  0.898701   \n",
       "\n",
       "                          T-5       T-6       T-7       T-8       T-9  \\\n",
       "timestamp                                                               \n",
       "2016-01-01 23:00:00  2.001836  1.445126  1.372633  1.309160  1.565804   \n",
       "2016-01-02 00:00:00  1.769037  2.001836  1.445126  1.372633  1.309160   \n",
       "2016-01-02 01:00:00  3.302904  1.769037  2.001836  1.445126  1.372633   \n",
       "2016-01-02 02:00:00  3.798601  3.302904  1.769037  2.001836  1.445126   \n",
       "2016-01-02 03:00:00  2.292393  3.798601  3.302904  1.769037  2.001836   \n",
       "\n",
       "                       ...         T-20      T-21      T-22      T-23  \\\n",
       "timestamp              ...                                              \n",
       "2016-01-01 23:00:00    ...    -1.025069 -0.581760  0.041002  0.617284   \n",
       "2016-01-02 00:00:00    ...    -1.291514 -1.025069 -0.581760  0.041002   \n",
       "2016-01-02 01:00:00    ...    -1.395559 -1.291514 -1.025069 -0.581760   \n",
       "2016-01-02 02:00:00    ...    -1.380255 -1.395559 -1.291514 -1.025069   \n",
       "2016-01-02 03:00:00    ...    -1.233514 -1.380255 -1.395559 -1.291514   \n",
       "\n",
       "                          T+1       T+2       T+3       T+4       T+5  \\\n",
       "timestamp                                                               \n",
       "2016-01-01 23:00:00  0.113880 -0.547334 -0.905009 -1.132992 -1.258555   \n",
       "2016-01-02 00:00:00 -0.547334 -0.905009 -1.132992 -1.258555 -1.311624   \n",
       "2016-01-02 01:00:00 -0.905009 -1.132992 -1.258555 -1.311624 -1.219801   \n",
       "2016-01-02 02:00:00 -1.132992 -1.258555 -1.311624 -1.219801 -0.935167   \n",
       "2016-01-02 03:00:00 -1.258555 -1.311624 -1.219801 -0.935167 -0.475324   \n",
       "\n",
       "                          T+6  \n",
       "timestamp                      \n",
       "2016-01-01 23:00:00 -1.311624  \n",
       "2016-01-02 00:00:00 -1.219801  \n",
       "2016-01-02 01:00:00 -0.935167  \n",
       "2016-01-02 02:00:00 -0.475324  \n",
       "2016-01-02 03:00:00 -0.013263  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = OrderedDict()\n",
    "for i in range(PAST_TIME_RANGE):\n",
    "    col_name = \"T-{}\".format(i)\n",
    "    series = data.views.shift(i)\n",
    "    df[col_name] = series\n",
    "for i in range(1,FUTURE_TIME_RANGE+1):\n",
    "    col_name = \"T+{}\".format(i)\n",
    "    series = data.views.shift(-i)\n",
    "    df[col_name] = series\n",
    "data_set = pd.DataFrame(df, index=data.index).dropna()\n",
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4971, 24), (4971, 6))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data_set[[c for c in data_set.columns if \"-\" in c]]\n",
    "Y = data_set[[c for c in data_set.columns if \"+\" in c]]\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3728, 24), (1243, 24), (3728, 6), (1243, 6))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit_train_test = round(0.75 * len(X))\n",
    "x_train, x_test = X[:limit_train_test].values, X[limit_train_test:].values\n",
    "y_train, y_test = Y[:limit_train_test].values, Y[limit_train_test:].values\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv1d(1, 128, 5, padding=2)\n",
    "        self.cnn16 = nn.Conv1d(128, 128, 5, padding=2)\n",
    "        self.cnn_stride = nn.Conv1d(128, 128, 5, padding=2, stride=2)\n",
    "        self.linear = nn.Linear(128*12, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(FUTURE_TIME_RANGE):\n",
    "            output = self.cnn1(input.view(input.data.shape[0], 1, input.data.shape[1]))\n",
    "            output = self.cnn16(output)\n",
    "            output = self.cnn16(output)\n",
    "            output = self.cnn_stride(output)\n",
    "            output = self.cnn16(output)\n",
    "            output = self.cnn16(output)\n",
    "            output = self.linear(output.view(output.data.shape[0], -1))\n",
    "            outputs += [output]\n",
    "            input = torch.cat([input[:,1:], output], 1)\n",
    "        outputs = torch.cat(outputs, 1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Train 0.638 \t Test 0.3105 \t Best MSE 0.3105\n",
      "Epoch 1 : Train 0.5125 \t Test 0.3096 \t Best MSE 0.3096\n",
      "Epoch 2 : Train 0.5373 \t Test 0.3102 \t Best MSE 0.3096\n",
      "Epoch 3 : Train 0.5 \t Test 0.3084 \t Best MSE 0.3084\n",
      "Epoch 4 : Train 0.5345 \t Test 0.3047 \t Best MSE 0.3047\n",
      "Epoch 5 : Train 0.4932 \t Test 0.3074 \t Best MSE 0.3047\n",
      "Epoch 6 : Train 0.4614 \t Test 0.3062 \t Best MSE 0.3047\n",
      "Epoch 7 : Train 0.4625 \t Test 0.3057 \t Best MSE 0.3047\n",
      "Epoch 8 : Train 0.486 \t Test 0.3065 \t Best MSE 0.3047\n",
      "Epoch 9 : Train 0.4659 \t Test 0.3077 \t Best MSE 0.3047\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 10 : Train 0.4844 \t Test 0.3085 \t Best MSE 0.3047\n",
      "Epoch 11 : Train 0.3923 \t Test 0.3007 \t Best MSE 0.3007\n",
      "Epoch 12 : Train 0.3899 \t Test 0.2994 \t Best MSE 0.2994\n",
      "Epoch 13 : Train 0.3894 \t Test 0.2986 \t Best MSE 0.2986\n",
      "Epoch 14 : Train 0.3891 \t Test 0.2981 \t Best MSE 0.2981\n",
      "Epoch 15 : Train 0.389 \t Test 0.2977 \t Best MSE 0.2977\n",
      "Epoch 16 : Train 0.389 \t Test 0.2975 \t Best MSE 0.2975\n",
      "Epoch 17 : Train 0.389 \t Test 0.2973 \t Best MSE 0.2973\n",
      "Epoch 18 : Train 0.389 \t Test 0.2972 \t Best MSE 0.2972\n",
      "Epoch 19 : Train 0.3889 \t Test 0.2971 \t Best MSE 0.2971\n",
      "Epoch 20 : Train 0.3889 \t Test 0.297 \t Best MSE 0.297\n",
      "Epoch 21 : Train 0.3889 \t Test 0.297 \t Best MSE 0.297\n",
      "Epoch 22 : Train 0.3889 \t Test 0.2969 \t Best MSE 0.2969\n",
      "Epoch 23 : Train 0.3889 \t Test 0.2969 \t Best MSE 0.2969\n",
      "Epoch 24 : Train 0.3889 \t Test 0.2968 \t Best MSE 0.2968\n",
      "Epoch 25 : Train 0.3889 \t Test 0.2968 \t Best MSE 0.2968\n",
      "Epoch 26 : Train 0.3889 \t Test 0.2968 \t Best MSE 0.2968\n",
      "Epoch 27 : Train 0.3889 \t Test 0.2968 \t Best MSE 0.2968\n",
      "Epoch 28 : Train 0.3889 \t Test 0.2968 \t Best MSE 0.2968\n",
      "Epoch 29 : Train 0.3888 \t Test 0.2967 \t Best MSE 0.2967\n",
      "Epoch 30 : Train 0.3888 \t Test 0.2967 \t Best MSE 0.2967\n",
      "Epoch 31 : Train 0.3888 \t Test 0.2967 \t Best MSE 0.2967\n",
      "Epoch 32 : Train 0.3888 \t Test 0.2967 \t Best MSE 0.2967\n",
      "Epoch 33 : Train 0.3888 \t Test 0.2967 \t Best MSE 0.2967\n",
      "Epoch 34 : Train 0.3888 \t Test 0.2967 \t Best MSE 0.2967\n",
      "Epoch 35 : Train 0.3888 \t Test 0.2967 \t Best MSE 0.2967\n",
      "Epoch 36 : Train 0.3888 \t Test 0.2967 \t Best MSE 0.2967\n",
      "Epoch 37 : Train 0.3887 \t Test 0.2967 \t Best MSE 0.2967\n",
      "Epoch 38 : Train 0.3887 \t Test 0.2967 \t Best MSE 0.2967\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-161-257742ced865>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0moptimized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0moptimized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \"\"\"\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 98\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if type(x_train) != torch.DoubleTensor:\n",
    "    x_train = torch.from_numpy(x_train)\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    x_test = torch.from_numpy(x_test)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "\n",
    "model = Model().cuda()\n",
    "optimized = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = ReduceLROnPlateau(optimized, patience=5, verbose=True)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "best_mse = 999\n",
    "early_stop_counter = 0\n",
    "for epoch in range(128):\n",
    "    MSE_train = np.zeros(math.ceil(x_train.shape[0]/16))\n",
    "    MSE_test = np.zeros(math.ceil(x_test.shape[0]/16))\n",
    "\n",
    "    for i in range(0, x_train.shape[0], 16):\n",
    "        x_train_batch, y_train_batch = Variable(x_train[i:i+16]).float().cuda(), Variable(y_train[i:i+16]).float().cuda()\n",
    "        pred = model.forward(x_train_batch)\n",
    "        loss = criterion(pred, y_train_batch)\n",
    "        \n",
    "        MSE_train[int(i//16)] = loss.data[0]\n",
    "        \n",
    "        optimized.zero_grad()\n",
    "        loss.backward()\n",
    "        optimized.step()\n",
    "\n",
    "    # compute MSE for test\n",
    "    for i in range(0, x_test.shape[0], 16):\n",
    "        x_test_batch, y_test_batch = Variable(x_test[i:i+16]).float().cuda(), Variable(y_test[i:i+16]).float().cuda()\n",
    "        pred = model.forward(x_test_batch)\n",
    "        loss = criterion(pred, y_test_batch)\n",
    "        MSE_test[int(i//16)] = loss.data[0]\n",
    "    \n",
    "    scheduler.step(MSE_test.mean())\n",
    "    \n",
    "    if MSE_test.mean() < best_mse:\n",
    "        best_mse = MSE_test.mean()\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter +=1\n",
    "\n",
    "    print(\"Epoch {} : Train {:.4} \\t Test {:.4} \\t Best MSE {:.4}\".format(epoch, MSE_train.mean(), MSE_test.mean(), best_mse))\n",
    "    if early_stop_counter > 10:\n",
    "        print(\"early stopped\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nADAzqykHgJlZTTkAzMxqygFgZlZT/x+VHCIMhi55FQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b71c4afe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 22\n",
    "x = Variable(x_test[i:i+16]).float().cuda()\n",
    "y = Variable(y_test[i:i+16]).float().cuda()\n",
    "pred = model.forward(x)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(range(24), x.data[0].cpu().numpy())\n",
    "plt.scatter(range(24, 24+6), y.data[0].cpu().numpy())\n",
    "plt.scatter(range(24, 24+6), pred.data[0].cpu().numpy())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
