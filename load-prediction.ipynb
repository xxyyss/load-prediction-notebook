{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math \n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAST_TIME_RANGE = 24  # use 24 past timeframe\n",
    "FUTURE_TIME_RANGE = 6  # predict 6 timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"wikipedia.csv\", index_col=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>0.617284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>0.041002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>-0.581760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>-1.025069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>-1.291514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        views\n",
       "timestamp                    \n",
       "2016-01-01 00:00:00  0.617284\n",
       "2016-01-01 01:00:00  0.041002\n",
       "2016-01-01 02:00:00 -0.581760\n",
       "2016-01-01 03:00:00 -1.025069\n",
       "2016-01-01 04:00:00 -1.291514"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T-0</th>\n",
       "      <th>T-1</th>\n",
       "      <th>T-2</th>\n",
       "      <th>T-3</th>\n",
       "      <th>T-4</th>\n",
       "      <th>T-5</th>\n",
       "      <th>T-6</th>\n",
       "      <th>T-7</th>\n",
       "      <th>T-8</th>\n",
       "      <th>T-9</th>\n",
       "      <th>...</th>\n",
       "      <th>T-20</th>\n",
       "      <th>T-21</th>\n",
       "      <th>T-22</th>\n",
       "      <th>T-23</th>\n",
       "      <th>T+1</th>\n",
       "      <th>T+2</th>\n",
       "      <th>T+3</th>\n",
       "      <th>T+4</th>\n",
       "      <th>T+5</th>\n",
       "      <th>T+6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 23:00:00</th>\n",
       "      <td>0.898701</td>\n",
       "      <td>2.292393</td>\n",
       "      <td>3.798601</td>\n",
       "      <td>3.302904</td>\n",
       "      <td>1.769037</td>\n",
       "      <td>2.001836</td>\n",
       "      <td>1.445126</td>\n",
       "      <td>1.372633</td>\n",
       "      <td>1.309160</td>\n",
       "      <td>1.565804</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.025069</td>\n",
       "      <td>-0.581760</td>\n",
       "      <td>0.041002</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.113880</td>\n",
       "      <td>-0.547334</td>\n",
       "      <td>-0.905009</td>\n",
       "      <td>-1.132992</td>\n",
       "      <td>-1.258555</td>\n",
       "      <td>-1.311624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 00:00:00</th>\n",
       "      <td>0.113880</td>\n",
       "      <td>0.898701</td>\n",
       "      <td>2.292393</td>\n",
       "      <td>3.798601</td>\n",
       "      <td>3.302904</td>\n",
       "      <td>1.769037</td>\n",
       "      <td>2.001836</td>\n",
       "      <td>1.445126</td>\n",
       "      <td>1.372633</td>\n",
       "      <td>1.309160</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.291514</td>\n",
       "      <td>-1.025069</td>\n",
       "      <td>-0.581760</td>\n",
       "      <td>0.041002</td>\n",
       "      <td>-0.547334</td>\n",
       "      <td>-0.905009</td>\n",
       "      <td>-1.132992</td>\n",
       "      <td>-1.258555</td>\n",
       "      <td>-1.311624</td>\n",
       "      <td>-1.219801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 01:00:00</th>\n",
       "      <td>-0.547334</td>\n",
       "      <td>0.113880</td>\n",
       "      <td>0.898701</td>\n",
       "      <td>2.292393</td>\n",
       "      <td>3.798601</td>\n",
       "      <td>3.302904</td>\n",
       "      <td>1.769037</td>\n",
       "      <td>2.001836</td>\n",
       "      <td>1.445126</td>\n",
       "      <td>1.372633</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.395559</td>\n",
       "      <td>-1.291514</td>\n",
       "      <td>-1.025069</td>\n",
       "      <td>-0.581760</td>\n",
       "      <td>-0.905009</td>\n",
       "      <td>-1.132992</td>\n",
       "      <td>-1.258555</td>\n",
       "      <td>-1.311624</td>\n",
       "      <td>-1.219801</td>\n",
       "      <td>-0.935167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 02:00:00</th>\n",
       "      <td>-0.905009</td>\n",
       "      <td>-0.547334</td>\n",
       "      <td>0.113880</td>\n",
       "      <td>0.898701</td>\n",
       "      <td>2.292393</td>\n",
       "      <td>3.798601</td>\n",
       "      <td>3.302904</td>\n",
       "      <td>1.769037</td>\n",
       "      <td>2.001836</td>\n",
       "      <td>1.445126</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.380255</td>\n",
       "      <td>-1.395559</td>\n",
       "      <td>-1.291514</td>\n",
       "      <td>-1.025069</td>\n",
       "      <td>-1.132992</td>\n",
       "      <td>-1.258555</td>\n",
       "      <td>-1.311624</td>\n",
       "      <td>-1.219801</td>\n",
       "      <td>-0.935167</td>\n",
       "      <td>-0.475324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-02 03:00:00</th>\n",
       "      <td>-1.132992</td>\n",
       "      <td>-0.905009</td>\n",
       "      <td>-0.547334</td>\n",
       "      <td>0.113880</td>\n",
       "      <td>0.898701</td>\n",
       "      <td>2.292393</td>\n",
       "      <td>3.798601</td>\n",
       "      <td>3.302904</td>\n",
       "      <td>1.769037</td>\n",
       "      <td>2.001836</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.233514</td>\n",
       "      <td>-1.380255</td>\n",
       "      <td>-1.395559</td>\n",
       "      <td>-1.291514</td>\n",
       "      <td>-1.258555</td>\n",
       "      <td>-1.311624</td>\n",
       "      <td>-1.219801</td>\n",
       "      <td>-0.935167</td>\n",
       "      <td>-0.475324</td>\n",
       "      <td>-0.013263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          T-0       T-1       T-2       T-3       T-4  \\\n",
       "timestamp                                                               \n",
       "2016-01-01 23:00:00  0.898701  2.292393  3.798601  3.302904  1.769037   \n",
       "2016-01-02 00:00:00  0.113880  0.898701  2.292393  3.798601  3.302904   \n",
       "2016-01-02 01:00:00 -0.547334  0.113880  0.898701  2.292393  3.798601   \n",
       "2016-01-02 02:00:00 -0.905009 -0.547334  0.113880  0.898701  2.292393   \n",
       "2016-01-02 03:00:00 -1.132992 -0.905009 -0.547334  0.113880  0.898701   \n",
       "\n",
       "                          T-5       T-6       T-7       T-8       T-9  \\\n",
       "timestamp                                                               \n",
       "2016-01-01 23:00:00  2.001836  1.445126  1.372633  1.309160  1.565804   \n",
       "2016-01-02 00:00:00  1.769037  2.001836  1.445126  1.372633  1.309160   \n",
       "2016-01-02 01:00:00  3.302904  1.769037  2.001836  1.445126  1.372633   \n",
       "2016-01-02 02:00:00  3.798601  3.302904  1.769037  2.001836  1.445126   \n",
       "2016-01-02 03:00:00  2.292393  3.798601  3.302904  1.769037  2.001836   \n",
       "\n",
       "                       ...         T-20      T-21      T-22      T-23  \\\n",
       "timestamp              ...                                              \n",
       "2016-01-01 23:00:00    ...    -1.025069 -0.581760  0.041002  0.617284   \n",
       "2016-01-02 00:00:00    ...    -1.291514 -1.025069 -0.581760  0.041002   \n",
       "2016-01-02 01:00:00    ...    -1.395559 -1.291514 -1.025069 -0.581760   \n",
       "2016-01-02 02:00:00    ...    -1.380255 -1.395559 -1.291514 -1.025069   \n",
       "2016-01-02 03:00:00    ...    -1.233514 -1.380255 -1.395559 -1.291514   \n",
       "\n",
       "                          T+1       T+2       T+3       T+4       T+5  \\\n",
       "timestamp                                                               \n",
       "2016-01-01 23:00:00  0.113880 -0.547334 -0.905009 -1.132992 -1.258555   \n",
       "2016-01-02 00:00:00 -0.547334 -0.905009 -1.132992 -1.258555 -1.311624   \n",
       "2016-01-02 01:00:00 -0.905009 -1.132992 -1.258555 -1.311624 -1.219801   \n",
       "2016-01-02 02:00:00 -1.132992 -1.258555 -1.311624 -1.219801 -0.935167   \n",
       "2016-01-02 03:00:00 -1.258555 -1.311624 -1.219801 -0.935167 -0.475324   \n",
       "\n",
       "                          T+6  \n",
       "timestamp                      \n",
       "2016-01-01 23:00:00 -1.311624  \n",
       "2016-01-02 00:00:00 -1.219801  \n",
       "2016-01-02 01:00:00 -0.935167  \n",
       "2016-01-02 02:00:00 -0.475324  \n",
       "2016-01-02 03:00:00 -0.013263  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = OrderedDict()\n",
    "for i in range(PAST_TIME_RANGE):\n",
    "    col_name = \"T-{}\".format(i)\n",
    "    series = data.views.shift(i)\n",
    "    df[col_name] = series\n",
    "for i in range(1,FUTURE_TIME_RANGE+1):\n",
    "    col_name = \"T+{}\".format(i)\n",
    "    series = data.views.shift(-i)\n",
    "    df[col_name] = series\n",
    "data_set = pd.DataFrame(df, index=data.index).dropna()\n",
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4971, 24), (4971, 6))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data_set[[c for c in data_set.columns if \"-\" in c]]\n",
    "Y = data_set[[c for c in data_set.columns if \"+\" in c]]\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3728, 24), (1243, 24), (3728, 6), (1243, 6))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit_train_test = round(0.75 * len(X))\n",
    "x_train, x_test = X[:limit_train_test].values, X[limit_train_test:].values\n",
    "y_train, y_test = Y[:limit_train_test].values, Y[limit_train_test:].values\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv1d(1, 16, 5, padding=2)\n",
    "        self.cnn16 = nn.Conv1d(16, 16, 5, padding=2)\n",
    "        self.cnn_stride = nn.Conv1d(16, 16, 5, padding=2, stride=2)\n",
    "        self.deconv1 = nn.ConvTranspose1d(16*12, 16, 3)\n",
    "        self.deconv2 = nn.ConvTranspose1d(16, 16, 3)\n",
    "        self.deconv3 = nn.ConvTranspose1d(16, 1, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchNorm = nn.BatchNorm1d(16)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, input, train=True):\n",
    "        output = self.cnn1(input.view(input.data.shape[0], 1, input.data.shape[1]))\n",
    "        output = self.batchNorm(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.cnn16(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.cnn16(output)\n",
    "        output = self.relu(output)\n",
    "        \n",
    "        output = self.cnn_stride(output)\n",
    "        \n",
    "        output = self.relu(output)\n",
    "        output = self.cnn16(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.cnn16(output)\n",
    "        output = self.relu(output)\n",
    "        \n",
    "#         if train:\n",
    "#             output = self.dropout(output)\n",
    "        \n",
    "        output = self.deconv1(output.view(output.data.shape[0], -1, 1))\n",
    "        output = self.batchNorm(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.deconv2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.deconv3(output)[:,:,:-1]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Train 0.4273 \t Test 0.1063 \t Best MSE 0.1063\n",
      "Epoch 1 : Train 0.1142 \t Test 0.0901 \t Best MSE 0.0901\n",
      "Epoch 2 : Train 0.09596 \t Test 0.08311 \t Best MSE 0.08311\n",
      "Epoch 3 : Train 0.07519 \t Test 0.07279 \t Best MSE 0.07279\n",
      "Epoch 4 : Train 0.05714 \t Test 0.06576 \t Best MSE 0.06576\n",
      "Epoch 5 : Train 0.04717 \t Test 0.05901 \t Best MSE 0.05901\n",
      "Epoch 6 : Train 0.04118 \t Test 0.05481 \t Best MSE 0.05481\n",
      "Epoch 7 : Train 0.0367 \t Test 0.05378 \t Best MSE 0.05378\n",
      "Epoch 8 : Train 0.03346 \t Test 0.05444 \t Best MSE 0.05378\n",
      "Epoch 9 : Train 0.03089 \t Test 0.05474 \t Best MSE 0.05378\n",
      "Epoch 10 : Train 0.02953 \t Test 0.05395 \t Best MSE 0.05378\n",
      "Epoch 11 : Train 0.02816 \t Test 0.05225 \t Best MSE 0.05225\n",
      "Epoch 12 : Train 0.02694 \t Test 0.0525 \t Best MSE 0.05225\n",
      "Epoch 13 : Train 0.02614 \t Test 0.05127 \t Best MSE 0.05127\n",
      "Epoch 14 : Train 0.02515 \t Test 0.05183 \t Best MSE 0.05127\n",
      "Epoch 15 : Train 0.02455 \t Test 0.05122 \t Best MSE 0.05122\n",
      "Epoch 16 : Train 0.02392 \t Test 0.05193 \t Best MSE 0.05122\n",
      "Epoch 17 : Train 0.02341 \t Test 0.05067 \t Best MSE 0.05067\n",
      "Epoch 18 : Train 0.02266 \t Test 0.05188 \t Best MSE 0.05067\n",
      "Epoch 19 : Train 0.02231 \t Test 0.05123 \t Best MSE 0.05067\n",
      "Epoch 20 : Train 0.02172 \t Test 0.05078 \t Best MSE 0.05067\n",
      "Epoch 21 : Train 0.0213 \t Test 0.05001 \t Best MSE 0.05001\n",
      "Epoch 22 : Train 0.02024 \t Test 0.04896 \t Best MSE 0.04896\n",
      "Epoch 23 : Train 0.01995 \t Test 0.0464 \t Best MSE 0.0464\n",
      "Epoch 24 : Train 0.01918 \t Test 0.04555 \t Best MSE 0.04555\n",
      "Epoch 25 : Train 0.01861 \t Test 0.04631 \t Best MSE 0.04555\n",
      "Epoch 26 : Train 0.01852 \t Test 0.04656 \t Best MSE 0.04555\n",
      "Epoch 27 : Train 0.01843 \t Test 0.04594 \t Best MSE 0.04555\n",
      "Epoch 28 : Train 0.01752 \t Test 0.04601 \t Best MSE 0.04555\n",
      "Epoch 29 : Train 0.01726 \t Test 0.0463 \t Best MSE 0.04555\n",
      "Epoch 30 : Train 0.01711 \t Test 0.04496 \t Best MSE 0.04496\n",
      "Epoch 31 : Train 0.01718 \t Test 0.04486 \t Best MSE 0.04486\n",
      "Epoch 32 : Train 0.01671 \t Test 0.04468 \t Best MSE 0.04468\n",
      "Epoch 33 : Train 0.01665 \t Test 0.04429 \t Best MSE 0.04429\n",
      "Epoch 34 : Train 0.0163 \t Test 0.04372 \t Best MSE 0.04372\n",
      "Epoch 35 : Train 0.01659 \t Test 0.04395 \t Best MSE 0.04372\n",
      "Epoch 36 : Train 0.0166 \t Test 0.04391 \t Best MSE 0.04372\n",
      "Epoch 37 : Train 0.01634 \t Test 0.04448 \t Best MSE 0.04372\n",
      "Epoch 38 : Train 0.01673 \t Test 0.04391 \t Best MSE 0.04372\n",
      "Epoch 39 : Train 0.0158 \t Test 0.04398 \t Best MSE 0.04372\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 40 : Train 0.01596 \t Test 0.04439 \t Best MSE 0.04372\n",
      "Epoch 41 : Train 0.0152 \t Test 0.03993 \t Best MSE 0.03993\n",
      "Epoch 42 : Train 0.0144 \t Test 0.03988 \t Best MSE 0.03988\n",
      "Epoch 43 : Train 0.01391 \t Test 0.03985 \t Best MSE 0.03985\n",
      "Epoch 44 : Train 0.01358 \t Test 0.03983 \t Best MSE 0.03983\n",
      "Epoch 45 : Train 0.01327 \t Test 0.03972 \t Best MSE 0.03972\n",
      "Epoch 46 : Train 0.01299 \t Test 0.03965 \t Best MSE 0.03965\n",
      "Epoch 47 : Train 0.01275 \t Test 0.0396 \t Best MSE 0.0396\n",
      "Epoch 48 : Train 0.01255 \t Test 0.03953 \t Best MSE 0.03953\n",
      "Epoch 49 : Train 0.01235 \t Test 0.03944 \t Best MSE 0.03944\n",
      "Epoch 50 : Train 0.01216 \t Test 0.03935 \t Best MSE 0.03935\n",
      "Epoch 51 : Train 0.01199 \t Test 0.03933 \t Best MSE 0.03933\n",
      "Epoch 52 : Train 0.01183 \t Test 0.03932 \t Best MSE 0.03932\n",
      "Epoch 53 : Train 0.01168 \t Test 0.03935 \t Best MSE 0.03932\n",
      "Epoch 54 : Train 0.01153 \t Test 0.03938 \t Best MSE 0.03932\n",
      "Epoch 55 : Train 0.01139 \t Test 0.03941 \t Best MSE 0.03932\n",
      "Epoch 56 : Train 0.01126 \t Test 0.03933 \t Best MSE 0.03932\n",
      "Epoch 57 : Train 0.01114 \t Test 0.03934 \t Best MSE 0.03932\n",
      "Epoch 58 : Train 0.01101 \t Test 0.03928 \t Best MSE 0.03928\n",
      "Epoch 59 : Train 0.01089 \t Test 0.03922 \t Best MSE 0.03922\n",
      "Epoch 60 : Train 0.01079 \t Test 0.03919 \t Best MSE 0.03919\n",
      "Epoch 61 : Train 0.01069 \t Test 0.03911 \t Best MSE 0.03911\n",
      "Epoch 62 : Train 0.01059 \t Test 0.03913 \t Best MSE 0.03911\n",
      "Epoch 63 : Train 0.01049 \t Test 0.03906 \t Best MSE 0.03906\n",
      "Epoch 64 : Train 0.01041 \t Test 0.03909 \t Best MSE 0.03906\n",
      "Epoch 65 : Train 0.01031 \t Test 0.03901 \t Best MSE 0.03901\n",
      "Epoch 66 : Train 0.01023 \t Test 0.03904 \t Best MSE 0.03901\n",
      "Epoch 67 : Train 0.01014 \t Test 0.03897 \t Best MSE 0.03897\n",
      "Epoch 68 : Train 0.01006 \t Test 0.03895 \t Best MSE 0.03895\n",
      "Epoch 69 : Train 0.009984 \t Test 0.03891 \t Best MSE 0.03891\n",
      "Epoch 70 : Train 0.009907 \t Test 0.03889 \t Best MSE 0.03889\n",
      "Epoch 71 : Train 0.00983 \t Test 0.03887 \t Best MSE 0.03887\n",
      "Epoch 72 : Train 0.009767 \t Test 0.03883 \t Best MSE 0.03883\n",
      "Epoch 73 : Train 0.009697 \t Test 0.03879 \t Best MSE 0.03879\n",
      "Epoch 74 : Train 0.009639 \t Test 0.03878 \t Best MSE 0.03878\n",
      "Epoch 75 : Train 0.00957 \t Test 0.03874 \t Best MSE 0.03874\n",
      "Epoch 76 : Train 0.009512 \t Test 0.03868 \t Best MSE 0.03868\n",
      "Epoch 77 : Train 0.009449 \t Test 0.03872 \t Best MSE 0.03868\n",
      "Epoch 78 : Train 0.009387 \t Test 0.03875 \t Best MSE 0.03868\n",
      "Epoch 79 : Train 0.009337 \t Test 0.03872 \t Best MSE 0.03868\n",
      "Epoch 80 : Train 0.009277 \t Test 0.0387 \t Best MSE 0.03868\n",
      "Epoch 81 : Train 0.009227 \t Test 0.03872 \t Best MSE 0.03868\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 82 : Train 0.00917 \t Test 0.03871 \t Best MSE 0.03868\n",
      "Epoch 83 : Train 0.01036 \t Test 0.04131 \t Best MSE 0.03868\n",
      "Epoch 84 : Train 0.009748 \t Test 0.04181 \t Best MSE 0.03868\n",
      "Epoch 85 : Train 0.009641 \t Test 0.04201 \t Best MSE 0.03868\n",
      "Epoch 86 : Train 0.009585 \t Test 0.04209 \t Best MSE 0.03868\n",
      "Epoch 87 : Train 0.009545 \t Test 0.04213 \t Best MSE 0.03868\n",
      "early stopped\n"
     ]
    }
   ],
   "source": [
    "if type(x_train) != torch.DoubleTensor:\n",
    "    x_train = torch.from_numpy(x_train)\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    x_test = torch.from_numpy(x_test)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "\n",
    "model = Model().cuda()\n",
    "optimized = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = ReduceLROnPlateau(optimized, patience=5, verbose=True)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "best_mse = 999\n",
    "early_stop_counter = 0\n",
    "for epoch in range(256):\n",
    "    MSE_train = np.zeros(math.ceil(x_train.shape[0]/16))\n",
    "    MSE_test = np.zeros(math.ceil(x_test.shape[0]/16))\n",
    "\n",
    "    for i in range(0, x_train.shape[0], 16):\n",
    "        x_train_batch, y_train_batch = Variable(x_train[i:i+16]).float().cuda(), Variable(y_train[i:i+16]).float().cuda()\n",
    "        pred = model.forward(x_train_batch)\n",
    "        loss = criterion(pred, y_train_batch)\n",
    "        \n",
    "        MSE_train[int(i//16)] = loss.data[0]\n",
    "        \n",
    "        optimized.zero_grad()\n",
    "        loss.backward()\n",
    "        optimized.step()\n",
    "\n",
    "    # compute MSE for test\n",
    "    for i in range(0, x_test.shape[0], 16):\n",
    "        x_test_batch, y_test_batch = Variable(x_test[i:i+16]).float().cuda(), Variable(y_test[i:i+16]).float().cuda()\n",
    "        pred = model.forward(x_test_batch, train=False)\n",
    "        loss = criterion(pred, y_test_batch)\n",
    "        MSE_test[int(i//16)] = loss.data[0]\n",
    "    \n",
    "    scheduler.step(MSE_test.mean())\n",
    "    \n",
    "    if MSE_test.mean() < best_mse:\n",
    "        best_mse = MSE_test.mean()\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter +=1\n",
    "\n",
    "    print(\"Epoch {} : Train {:.4} \\t Test {:.4} \\t Best MSE {:.4}\".format(epoch, MSE_train.mean(), MSE_test.mean(), best_mse))\n",
    "    if early_stop_counter > 10:\n",
    "        print(\"early stopped\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wcp5TPFwMwfXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b706994ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 25\n",
    "x = Variable(x_test[i:i+16]).float().cuda()\n",
    "y = Variable(y_test[i:i+16]).float().cuda()\n",
    "pred = model.forward(x)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(range(24), x.data[0].cpu().numpy())\n",
    "plt.scatter(range(24, 24+6), y.data[0].cpu().numpy())\n",
    "plt.scatter(range(24, 24+6), pred.data[0].cpu().numpy())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
